import pandas as pd
data = pd.read_excel('../data/(CORRIGIDO)ep2_pln_train.xlsx')
data.head()





import string
import nltk
from nltk.tokenize import word_tokenize
from unidecode import unidecode  # Certifique-se de instalar com: pip install unidecode
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# Baixe os recursos necessários do NLTK
nltk.download('punkt')

def analise_pontuacao_acentuacao(dataframe, coluna_texto):
    # Obter os textos da coluna especificada
    textos = dataframe[coluna_texto].astype(str)

    # Inicializar contadores
    contagem_pontuacao = Counter()
    contagem_acentuacao = Counter()

    # Iterar sobre os textos
    for texto in textos:
        # Tokenização
        tokens = word_tokenize(texto, language='portuguese')

        # Análise de Pontuação
        pontuacao = [char for char in tokens if char in string.punctuation]
        contagem_pontuacao.update(pontuacao)

        # Análise de Acentuação
        texto_sem_acentos = unidecode(texto)
        acentuacao = [char for char in texto if char.isalpha() and char != unidecode(char)]
        contagem_acentuacao.update(acentuacao)

    return contagem_pontuacao, contagem_acentuacao

def plotar_grafico(contagem, titulo):
    labels, valores = zip(*contagem.items())
    plt.bar(labels, valores)
    plt.title(titulo)
    plt.show()


a1 = data[data['age'] == 'a1']
a2 = data[data['age'] == 'a2']
a3 = data[data['age'] == 'a3']
a4 = data[data['age'] == 'a4']


# Executar a análise para a coluna 'Texto'
contagem_pontuacao_a1, contagem_acentuacao_a1 = analise_pontuacao_acentuacao(a1, 'req_text')
contagem_pontuacao_a2, contagem_acentuacao_a2 = analise_pontuacao_acentuacao(a2, 'req_text')
contagem_pontuacao_a3, contagem_acentuacao_a3 = analise_pontuacao_acentuacao(a3, 'req_text')
contagem_pontuacao_a4, contagem_acentuacao_a4 = analise_pontuacao_acentuacao(a4, 'req_text')


# Plotar gráficos
plotar_grafico(contagem_pontuacao_a1, "Análise de Pontuação a1")
plotar_grafico(contagem_pontuacao_a2, "Análise de Pontuação a2")
plotar_grafico(contagem_pontuacao_a3, "Análise de Pontuação a3")
plotar_grafico(contagem_pontuacao_a4, "Análise de Pontuação a4")


plotar_grafico(contagem_acentuacao_a1, "Análise de Acentuação a1")
plotar_grafico(contagem_acentuacao_a2, "Análise de Acentuação a2")
plotar_grafico(contagem_acentuacao_a3, "Análise de Acentuação a3")
plotar_grafico(contagem_acentuacao_a4, "Análise de Acentuação a4")





import pandas as pd

def calcular_diversidade_lexical_media(dataframe, coluna_texto):
    # Obter os textos da coluna especificada
    textos = dataframe[coluna_texto].astype(str)

    # Inicializar lista para armazenar as diversidades de cada texto
    diversidades = []

    # Iterar sobre os textos
    for texto in textos:
        # Tokenização
        palavras = texto.split()

        # Número total de palavras
        total_palavras = len(palavras)

        # Número de palavras únicas (tamanho do vocabulário)
        vocabulario = set(palavras)
        tamanho_vocabulario = len(vocabulario)

        # Razão de Tamanho do Vocabulário para o Número Total de Palavras (Type-Token Ratio)
        diversidade_lexical = tamanho_vocabulario / total_palavras

        diversidades.append(diversidade_lexical)

    # Calcular a média das diversidades
    diversidade_media = sum(diversidades) / len(diversidades) if diversidades else 0

    return diversidade_media


calcular_diversidade_lexical_media(a1, 'req_text')


calcular_diversidade_lexical_media(a2, 'req_text')


calcular_diversidade_lexical_media(a3, 'req_text')


calcular_diversidade_lexical_media(a4, 'req_text')





import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize

# Baixe os recursos necessários do NLTK
nltk.download('punkt')

def calcular_comprimento_medio_e_desvio_padrao_sentencas(dataframe, coluna_texto):
    # Obter os textos da coluna especificada
    textos = dataframe[coluna_texto].astype(str)

    # Inicializar lista para armazenar os comprimentos de sentenças de cada texto
    comprimentos_sentencas = []

    # Iterar sobre os textos
    for texto in textos:
        # Tokenização em sentenças
        sentencas = sent_tokenize(texto)

        # Calcular o comprimento de cada sentença e armazenar na lista
        comprimentos_sentencas.extend([len(sentenca.split()) for sentenca in sentencas])

    # Calcular o comprimento médio de sentenças e desvio-padrão
    comprimento_medio = sum(comprimentos_sentencas) / len(comprimentos_sentencas) if comprimentos_sentencas else 0
    desvio_padrao = (sum((x - comprimento_medio) ** 2 for x in comprimentos_sentencas) / len(comprimentos_sentencas)) ** 0.5

    return comprimento_medio, desvio_padrao


comprimento_medio, desvio_padrao = calcular_comprimento_medio_e_desvio_padrao_sentencas(a1, 'req_text')

print(f"Comprimento Médio de Sentenças: {comprimento_medio:.4f}")
print(f"Desvio-Padrão do Comprimento de Sentenças: {desvio_padrao:.4f}")


comprimento_medio, desvio_padrao = calcular_comprimento_medio_e_desvio_padrao_sentencas(a2, 'req_text')

print(f"Comprimento Médio de Sentenças: {comprimento_medio:.4f}")
print(f"Desvio-Padrão do Comprimento de Sentenças: {desvio_padrao:.4f}")


comprimento_medio, desvio_padrao = calcular_comprimento_medio_e_desvio_padrao_sentencas(a3, 'req_text')

print(f"Comprimento Médio de Sentenças: {comprimento_medio:.4f}")
print(f"Desvio-Padrão do Comprimento de Sentenças: {desvio_padrao:.4f}")


comprimento_medio, desvio_padrao = calcular_comprimento_medio_e_desvio_padrao_sentencas(a4, 'req_text')

print(f"Comprimento Médio de Sentenças: {comprimento_medio:.4f}")
print(f"Desvio-Padrão do Comprimento de Sentenças: {desvio_padrao:.4f}")





import pandas as pd
import nltk
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from collections import Counter

# Baixe os recursos necessários do NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def analisar_classes_gramaticais(dataframe, coluna_texto):
    # Obter os textos da coluna especificada
    textos = dataframe[coluna_texto].astype(str)

    # Inicializar um contador para cada classe gramatical
    contagem_classes_gramaticais = Counter()

    # Iterar sobre os textos
    for texto in textos:
        # Tokenização em palavras
        palavras = word_tokenize(texto)

        # Obter as classes gramaticais usando o pos_tag do NLTK
        classes_gramaticais = pos_tag(palavras)

        # Contar a ocorrência de cada classe gramatical
        contagem_classes_gramaticais.update(tag for _, tag in classes_gramaticais)

    return contagem_classes_gramaticais


# Analisar classes gramaticais para a coluna 'Texto'
contagem_classes_gramaticais = analisar_classes_gramaticais(a1, 'req_text')

# Exibir resultado
print("Contagem de Classes Gramaticais:")
for classe, contagem in contagem_classes_gramaticais.items():
    print(f"{classe}: {contagem}")


contagem_classes_gramaticais = analisar_classes_gramaticais(a2, 'req_text')

# Exibir resultado
print("Contagem de Classes Gramaticais:")
for classe, contagem in contagem_classes_gramaticais.items():
    print(f"{classe}: {contagem}")


contagem_classes_gramaticais = analisar_classes_gramaticais(a3, 'req_text')

# Exibir resultado
print("Contagem de Classes Gramaticais:")
for classe, contagem in contagem_classes_gramaticais.items():
    print(f"{classe}: {contagem}")


contagem_classes_gramaticais = analisar_classes_gramaticais(a4, 'req_text')

# Exibir resultado
print("Contagem de Classes Gramaticais:")
for classe, contagem in contagem_classes_gramaticais.items():
    print(f"{classe}: {contagem}")


import pandas as pd
import spacy

# Carregue o modelo de linguagem em português do spaCy
nlp = spacy.load("pt_core_news_sm")

def analisar_estrutura_frases(dataframe, coluna_texto):
    # Obter os textos da coluna especificada
    textos = dataframe[coluna_texto].astype(str)

    # Inicializar lista para armazenar métricas
    complexidades_sintaticas = []

    # Iterar sobre os textos
    for texto in textos:
        # Processar o texto com spaCy
        doc = nlp(texto)

        # Converter o gerador para uma lista para calcular o comprimento
        tokens = list(doc)

        # Calcular a complexidade sintática (número médio de dependências por palavra)
        complexidade_sintatica = sum(len(token.children) for token in tokens) / len(tokens) if len(tokens) > 0 else 0
        complexidades_sintaticas.append(complexidade_sintatica)

    # Calcular a média das complexidades sintáticas
    media_complexidades_sintaticas = sum(complexidades_sintaticas) / len(complexidades_sintaticas) if complexidades_sintaticas else 0

    return media_complexidades_sintaticas

# Exemplo de DataFrame
dados = {'Texto': ["Isso é um exemplo de texto. Uma frase aqui.", "Outro exemplo mais longo. Contém mais palavras e frases.", "Texto curto."]}
df = pd.DataFrame(dados)

# Analisar a estrutura de frases para a coluna 'Texto'
media_complexidades_sintaticas = analisar_estrutura_frases(df, 'Texto')

# Exibir resultado
print(f"Média da Complexidade Sintática: {media_complexidades_sintaticas:.2f}")

