
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier 
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import LinearSVC
import nltk
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import RFECV, RFE, SelectKBest
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn import set_config
set_config(display='diagram')
from sklearn.svm import SVC
import datetime
from sklearn.ensemble import StackingClassifier

import shap


#imports
from time import time
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.svm import SVC
#from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
import optuna
from sklearn.metrics import f1_score


import pandas as pd 

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import MaxAbsScaler
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import  classification_report
from sklearn.feature_selection import SelectPercentile


import eli5


shap.initjs()


data = pd.read_csv('../../../../data/processed/(CORRIGIDO)ep2_pln_train_pos_pt_core_news_lg.xlsx')
data.drop(['Unnamed: 0'], axis=1, inplace=True)
data.head()


data['age'] = data['age'].map({
                                'a4':3,
                                'a3': 2,
                                'a2': 1,
                                'a1': 0})


data.head()


best_params = {'vect__ngram_range': (1, 3),
               'selection__percentile': 100,
               'estimator__fit_prior': False,
               'estimator__alpha': 0.3}


pipeline = Pipeline([
                ('vect', TfidfVectorizer()),
                ('scaling', MaxAbsScaler()), 
                ('selection', SelectPercentile()),
                ('estimator', MultinomialNB())
                ])


best_params_xg = {'vect__ngram_range': (1, 4),
               'vect__analyzer': 'char',
               'selection__percentile': 66,
               'estimator__lambda': 1,
                'estimator__gamma': 7,
                'estimator__colsample_bytree': 1,
                'estimator__alpha': 31}


pipeline_xg = Pipeline([
                ('vect', TfidfVectorizer()),
                ('scaling', MaxAbsScaler()), 
                ('selection', SelectPercentile()),
                #('ros', RandomOverSampler(random_state=42)),
                ('estimator', XGBClassifier(seed=42))
                ])



xgb = pipeline_xg.set_params(**best_params_xg)


xgb.get_params


xgb.fit(X_train_text, y_train)


# Utilizando o eli5 para explicar o classificador XGBClassifier
# Criando uma instância do eli5 para o classificador
eli5_explainer = eli5.sklearn.Explanation(estimator=xgb.named_steps['estimator'],
                                          vec=xgb.named_steps['vect'])

# Obtendo uma explicação para uma instância de teste (índice 0 neste exemplo)
eli5_explanation = eli5.explain_prediction_text(xgb.named_steps['estimator'], X_test_text[0], vec=xgb.named_steps['vect'])


def get_classification(X_test_text):

    return xgb.predict(X_test_text)


# Instância do Explainer do SHAP para o modelo treinado
explainer = shap.Explainer(get_classification, X_train_text)
explainer.shap_values(X_test_text)


X_test_text.to_array()


# Instância do Explainer do SHAP para o modelo treinado
explainer = shap.Explainer(xgb.named_steps['estimator'])

# Obtendo as previsões SHAP para as instâncias de teste
shap_values = explainer.shap_values(X_test_text)


# Utilizando o eli5 para explicar o classificador XGBClassifier
eli5.show_weights(xgb.named_steps['estimator'], vec=xgb.named_steps['vect'])


type(X_test_text)


# importing the libraries
import lime
import sklearn.ensemble
from __future__ import print_function
from lime import lime_text
from sklearn.pipeline import make_pipeline
from lime.lime_text import LimeTextExplainer

# converting the vectoriser and model into a pipeline
# this is necessary as LIME takes a model pipeline as an input
#c = make_pipeline(tf, model)

# saving a list of strings version of the X_test object
ls_X_test= list(X_test_text)

# saving the class names in a dictionary to increase interpretability
class_names = {0: 'a1', 1:'a2', 2: 'a3', 3: 'a4'}


pd.DataFrame(X_train_text).columns


# sampling data from the training and test set to reduce time-taken
X_train_sample = shap.sample(X_train_text, 200)
#X_test_sample = shap.sample(X_test_tf, 40)


# creating the KernelExplainer using the logistic regression model and training sample
SHAP_explainer = shap.TreeExplainer(xgb.predict, X_train_sample)
# calculating the shap values of the test sample using the explainer 
shap_vals = SHAP_explainer.shap_values(X_test_text)





# create the LIME explainer
# add the class names for interpretability
LIME_explainer = LimeTextExplainer(class_names=class_names)

# choose a random single prediction
idx = 15
# explain the chosen prediction 
# use the probability results of the logistic regression
# can also add num_features parameter to reduce the number of features explained
LIME_exp = LIME_explainer.explain_instance(ls_X_test[idx], xgb.predict_proba)
# print results
print('Document id: %d' % idx)
print('Tweet: ', ls_X_test[idx])
print('Probability disaster =', xgb.predict_proba([ls_X_test[idx]]).round(3)[0,1])
print('True class: %s' % class_names.get(list(y_test)[idx]))











from eli5.lime import TextExplainer


xgb.predict_proba


exp = TextExplainer(random_state=42)


X_test_text.values[1]


exp.fit(X_test_text.values[0], xgb.predict_proba)








explainer = shap.TreeExplainer(xgb.named_steps['estimator'])
explainer.shap_values(X_test_text[:3])


eli5.show_weights(xgb, top=9)











def get_classification(X_test_text):

    return text_clf.predict(X_test_text)


get_classification


text_clf = pipeline.set_params(**best_params)


# a seguir os dados serão divididos entre features (X) e label (y)
X_text = data['req_text'] # features
y = data['age'] # label


X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42,
                                                   stratify=y)


text_clf.fit(X_train_text, y_train)


y_pred = text_clf.predict(X_test_text)
accuracy_score(y_test, y_pred)


# Utilizando o eli5 para explicar o classificador MultinomialNB
eli5.show_weights(pipeline.named_steps['estimator'], vec=pipeline.named_steps['vect'])


eli5.show_weights(text_clf, top=9)


X_test_text.iloc[:3]


# Instância do Explainer do SHAP para o modelo treinado
explainer = shap.Explainer(get_classification, X_train_text)
explainer.shap_values(X_test_text.iloc[:3])
