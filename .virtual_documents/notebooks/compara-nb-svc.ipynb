


import pandas as pd 

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import MaxAbsScaler
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import  classification_report


#imports
from time import time
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
import optuna



from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier 
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
import nltk
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import RFECV, RFE, SelectKBest
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn import set_config
set_config(display='diagram')
from sklearn.svm import SVC
import datetime





data = pd.read_csv('../data/processed/ep1_esic2023_clareza_TRAIN_pos_pt_core_news_lg.csv', index_col=0)


data.head() # visualização das primeiras 5 linhas do dataframe


data['pos'] = data['pos'].apply(lambda x: x[6:])


data.head()


data.shape # visualização do formato do dataframe


data.head()


data.tail()


data['clarity'] = data['clarity'].map({
                                    'c5':2,
                                    'c234': 1,
                                    'c1': 0})


data.head()


data.tail()


# a seguir os dados serão divididos entre features (X) e label (y)
X = data[['resp_text', 'pos']] # features
y = data['clarity'] # label








import warnings
warnings.filterwarnings('ignore')


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,
                                                   stratify=y)





vectorizer = CountVectorizer(analyzer= 'word') 
preprocessor = ColumnTransformer(transformers=[('vect_resp_text', vectorizer, 'resp_text'), 
                                                ('vect_pos', vectorizer, 'pos')])
scaler = MaxAbsScaler()
model = LogisticRegression(penalty='l1')


pipeline = Pipeline([
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            ('estimator', model)
            ])


param_grid = {
            "preprocessor__vect_resp_text__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "preprocessor__vect_pos__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "estimator__C": np.linspace(0,10,100),
            "estimator__solver": ['liblinear','saga']
            }


random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, random_state=42,
                                   cv=StratifiedKFold(n_splits=5),
                                  n_iter=3, verbose=4)


lg1_count = random_search.fit(X_train, y_train)


lg1_count.best_params_


result = cross_val_score(lg1_count, X, y, scoring='accuracy', cv=10, verbose=4).mean()
print('Após random search')
print(f'acc = {result}%')


vectorizer = TfidfVectorizer(analyzer= 'word')
preprocessor = ColumnTransformer(transformers=[('vect_resp_text', vectorizer, 'resp_text'), 
                                                ('vect_pos', vectorizer, 'pos')])
scaler = MaxAbsScaler()
model = LogisticRegression(penalty='l1')


pipeline = Pipeline([
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            ('estimator', model)
            ])


param_grid = {
            "preprocessor__vect_resp_text__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "preprocessor__vect_pos__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "estimator__C": np.linspace(0,10,1000),
            "estimator__solver": ['liblinear','saga']
            }


random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, random_state=42, 
                                   cv=StratifiedKFold(n_splits=5), n_iter=57, verbose=4)


lg1_tf = random_search.fit(X_train, y_train)


lg1_tf.best_params_


lg1_tf = {
        'preprocessor__vect_resp_text__ngram_range': (4, 4),
         'preprocessor__vect_pos__ngram_range': (2, 2),
         'estimator__solver': 'saga',
         'estimator__C': 8.428428428428429
}


pipeline = Pipeline([
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            ('estimator', model)
            ])


pipeline = pipeline.set_params(**lg1_tf) 


pipeline.get_params


result = cross_val_score(pipeline, X, y, scoring='accuracy', cv=10, verbose=4, n_jobs=-1).mean()
#acc = round(acc * 100, 2)
print('Após random search')
print(f'acc = {result}%')





result = cross_val_score(lg1_tf, X, y, scoring='accuracy', cv=10, verbose=4).mean()
print('Após random search')
print(f'acc = {result}%')


def objective_lg1(trial, vectorizer, scaler, selection):

    from sklearn.compose import ColumnTransformer

    preprocessor = ColumnTransformer(transformers=[('vect_resp_text', vectorizer, 'resp_text'), 
                                                  ('vect_pos', vectorizer, 'pos')])
    
    params = { 
        "estimator__C": trial.suggest_float("estimator__C", 0, 10), 
        "estimator__solver": trial.suggest_categorical("estimator__solver", ['liblinear','saga']),
        #"preprocessor__vect__ngram_range": trial.suggest_categorical("vect__ngram_range", [(1,1), (1,2), (1,3) ,(1,4)]),
        #"preprocessor__vect__analyzer": 'word'
        
        "preprocessor__vect_resp_text__ngram_range": trial.suggest_categorical("preprocessor__vect_resp_text__ngram_range", [(1,1), (1,2), (1,3) ,(1,4)]),
        "preprocessor__vect_resp_text__analyzer": 'word',
        
        "preprocessor__vect_pos__ngram_range": trial.suggest_categorical("preprocessor__vect_pos__ngram_range", [(1,1), (1,2), (1,3) ,(1,4)]),
        "preprocessor__vect_pos__analyzer": 'word'
    }
    
    model = LogisticRegression(penalty='l1')
    
    pipeline = Pipeline([
        
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            #('selection', selection),
            ('estimator', model)
        ])
    
    pipeline = pipeline.set_params(**params) 
    
    score =cross_val_score(pipeline, X, y, cv=StratifiedKFold(n_splits=5))
    accuracy = score.mean()
    
    return accuracy


from sklearn.compose import make_column_transformer

t0 = time()
study = optuna.create_study(direction='maximize')

vectorizer = CountVectorizer() 
#transformer = make_column_transformer((vectorizer, 'resp_text'), (vectorizer, 'pos'))
scaler = MaxAbsScaler()
selection = RFECV(estimator = RandomForestClassifier(), step = 30)

study.optimize(lambda trial: objective_lg1(trial,vectorizer, scaler, selection), n_trials=1, timeout=5850, n_jobs=-1, show_progress_bar=True)


print("-----------------------------------------------------------------------------")
print("Pronto em  %0.3fs" % (time() - t0))

print("Melhor score: %0.3f" % study.best_trial.value)
print("Melhores parametros encontrados:")
print(study.best_trial.params)


# Não está funcionando direito

t0 = time()
study = optuna.create_study(direction='maximize')

vectorizer = TfidfVectorizer()

study.optimize(lambda trial: objective_lg1(trial,vectorizer, scaler, selection), n_trials=None, timeout=5850, n_jobs=-1, show_progress_bar=True)


print("-----------------------------------------------------------------------------")
print("Pronto em  %0.3fs" % (time() - t0))

print("Melhor score: %0.3f" % study.best_trial.value)
print("Melhores parametros encontrados:")
print(study.best_trial.params)





vectorizer = CountVectorizer() 
preprocessor = ColumnTransformer(transformers=[('vect_resp_text', vectorizer, 'resp_text'), 
                                                ('vect_pos', vectorizer, 'pos')])
scaler = MaxAbsScaler()
model = LogisticRegression(penalty='l2')


pipeline = Pipeline([
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            ('estimator', model)
            ])


param_grid = {
            "preprocessor__vect_resp_text__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "preprocessor__vect_pos__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "estimator__C": np.linspace(0,10,100),
            "estimator__solver": ['lbfgs', 'newton-cg', 'newton-cholesky',
                                  'liblinear','saga', 'sag']
            }


random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, random_state=42,
                                   cv=StratifiedKFold(n_splits=5),
                                  n_iter=16, verbose=4)


lg2_count = random_search.fit(X_train, y_train)


lg2_count.best_params_


result = cross_val_score(lg2_count, X, y, scoring='accuracy', cv=10, verbose=4).mean()
print('Após random search')
print(f'acc = {result}%')





vectorizer = TfidfVectorizer(analyzer= 'word')
preprocessor = ColumnTransformer(transformers=[('vect_resp_text', vectorizer, 'resp_text'), 
                                                ('vect_pos', vectorizer, 'pos')])
scaler = MaxAbsScaler()
model = LogisticRegression(penalty='l2')


pipeline = Pipeline([
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            ('estimator', model)
            ])


param_grid = {
            "preprocessor__vect_resp_text__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "preprocessor__vect_pos__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "estimator__C": np.linspace(0,10,100),
            "estimator__solver": ['lbfgs', 'newton-cg', 'newton-cholesky',
                                  'liblinear','saga', 'sag']
            }


random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, random_state=42,
                                   cv=StratifiedKFold(n_splits=5),
                                  n_iter=16, verbose=4)


lg2_tf = random_search.fit(X_train, y_train)


lg2_tf.best_params_


result = cross_val_score(lg2_tf, X, y, scoring='accuracy', cv=10, verbose=4).mean()
print('Após random search')
print(f'acc = {result}%')


def objective_lg2(trial, vectorizer, scaler, selection):
    
    
    params = { 
        "estimator__C": trial.suggest_float("estimator__C", 0, 10), 
        "estimator__solver": trial.suggest_categorical("estimator__solver", ['liblinear','saga']),
        "vect__ngram_range": trial.suggest_categorical("vect__ngram_range", [(1,1), (1,2), (1,3) ,(1,4)]),
        "vect__analyzer": trial.suggest_categorical("vect__analyzer", ['word'])
    }
    
    model = LogisticRegression(penalty='l2') # penalização l2
    
    pipeline = Pipeline([
        
            ('vect', vectorizer), 
            ('scaling', scaler), 
            #('selection', selection),
            ('estimator', model)
        ])
    
    pipeline = pipeline.set_params(**params)   
    
    score =cross_val_score(pipeline, X, y, cv=StratifiedKFold(n_splits=5))
    accuracy = score.mean()
    
    return accuracy





t0 = time()
study = optuna.create_study(direction='maximize')

vectorizer = CountVectorizer() 
scaler = MaxAbsScaler()
selection = RFECV(estimator = RandomForestClassifier(), step = 30)

study.optimize(lambda trial: objective_lg2(trial,vectorizer, scaler, selection), n_trials=None, timeout=5850, n_jobs=-1, show_progress_bar=True)


print("-----------------------------------------------------------------------------")
print("Pronto em  %0.3fs" % (time() - t0))

print("Melhor score: %0.3f" % study.best_trial.value)
print("Melhores parametros encontrados:")
print(study.best_trial.params)





t0 = time()
study = optuna.create_study(direction='maximize')

vectorizer = TfidfVectorizer()

study.optimize(lambda trial: objective_lg2(trial,vectorizer, scaler, selection), n_trials=None, timeout=5850, n_jobs=-1, show_progress_bar=True)


print("-----------------------------------------------------------------------------")
print("Pronto em  %0.3fs" % (time() - t0))

print("Melhor score: %0.3f" % study.best_trial.value)
print("Melhores parametros encontrados:")
print(study.best_trial.params)


vectorizer = CountVectorizer(analyzer= 'word') 
preprocessor = ColumnTransformer(transformers=[('vect_resp_text', CountVectorizer(analyzer= 'word') , 'resp_text'), 
                                                ('vect_pos', CountVectorizer(analyzer= 'word') , 'pos')])
scaler = MaxAbsScaler()
model = SVC()


print(SVC().get_params().keys())


pipeline = Pipeline([
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            ('estimator', model)
            ])


param_grid = {
            "preprocessor__vect_resp_text__ngram_range": [(1,1), (2,2), (3,3), (4,4)],
            "preprocessor__vect_pos__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
            "estimator__gamma": [1, 0.1, 0.01, 0.001, 0.0001],
            "estimator__kernel": ['linear', 'poly', 'rbf', 'sigmoid'],
            "estimator__C": [0.1, 1, 10, 100, 1000]
            }


random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, random_state=42,
                                   cv=StratifiedKFold(n_splits=5),
                                  n_iter=16, verbose=4, n_jobs=-1)


svc_count = random_search.fit(X_train, y_train)


svc_count.best_params_


svc_count = {'preprocessor__vect_resp_text__ngram_range': (4, 4),
 'preprocessor__vect_pos__ngram_range': (2, 3),
 'estimator__kernel': 'sigmoid',
 'estimator__gamma': 0.0001,
 'estimator__C': 1000}

pipeline = Pipeline([
            ('preprocessor', preprocessor), 
            ('scaling', scaler), 
            ('estimator', model)
            ])

pipeline = pipeline.set_params(**svc_count) 
print(pipeline.get_params)


result = cross_val_score(pipeline, X, y, scoring='accuracy', cv=10, verbose=4, n_jobs=-1).mean()
print('Após random search')
print(f'acc = {result}%')


t0 = time()
study = optuna.create_study(direction='maximize')

vectorizer = CountVectorizer() 
scaler = MaxAbsScaler()
selection = RFECV(estimator = RandomForestClassifier(), step = 30)

study.optimize(lambda trial: objective_rf(trial,vectorizer, scaler, selection), n_trials=1, timeout=11700, n_jobs=-1, show_progress_bar=True)





print("-----------------------------------------------------------------------------")
print("Pronto em  %0.3fs" % (time() - t0))

print("Melhor score: %0.3f" % study.best_trial.value)
print("Melhores parametros encontrados:")
print(study.best_trial.params)


t0 = time()
study = optuna.create_study(direction='maximize')

vectorizer = TfidfVectorizer()


study.optimize(lambda trial: objective_rf(trial,vectorizer, scaler, selection), n_trials=1, timeout=11700, n_jobs=-1, show_progress_bar=True)


print("-----------------------------------------------------------------------------")
print("Pronto em  %0.3fs" % (time() - t0))

print("Melhor score: %0.3f" % study.best_trial.value)
print("Melhores parametros encontrados:")
print(study.best_trial.params)





def compara(iteracoes):
    
    # Escrita no arquivo
    with open('compara-nb-svc.txt', "w") as arquivo:
        pass
    
    modelos = [MultinomialNB(fit_prior= False)]
    
    vect = [CountVectorizer(analyzer='word'), TfidfVectorizer(analyzer= 'word')]
    
    for model in modelos:
        
        for vectorizer in vect:
    
            param_grid = None
            
            scaler = MaxAbsScaler()

            preprocessor = ColumnTransformer(transformers=[
                                            ('vect_resp_text', vectorizer,'resp_text'), 
                                            ('vect_pos', vectorizer,'pos')])

            pipeline = Pipeline([
                    ('preprocessor', preprocessor), 
                    ('scaling', scaler), 
                    ('estimator', model)
                    ])

            if isinstance(model, MultinomialNB):
                param_grid = {
                "preprocessor__vect_resp_text__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "preprocessor__vect_pos__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "estimator__alpha": [50, 15, 10, 5, 1, 0.5, 0.3, 0.1, 0.05, 0.03, 0.02, 0.01,  0.001],
                #"estimator__fit_prior": [True, False],
                }

            if isinstance(model, SVC):
                param_grid = {
                "preprocessor__vect_resp_text__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "preprocessor__vect_pos__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "estimator__gamma": [1, 0.1, 0.01, 0.001],
                "estimator__kernel": ['linear', 'sigmoid'],
                "estimator__C": [0.1, 1, 10, 100]
                }

            if isinstance(model, LogisticRegression):

                if 'l1' in model.get_params()['penalty']:
                    param_grid = {
                    "preprocessor__vect_resp_text__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                    "preprocessor__vect_pos__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                    "estimator__C": np.linspace(0,10,100),
                    "estimator__solver": ['liblinear','saga']
                    }

                elif 'l2' in model.get_params()['penalty']:
                    param_grid = {
                    "preprocessor__vect_resp_text__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                    "preprocessor__vect_pos__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                    "estimator__C": np.linspace(0,10,100),
                    "estimator__solver": ['lbfgs', 'newton-cg', 'newton-cholesky',
                                          'liblinear','saga', 'sag']
                    }

            if isinstance(model, RandomForestClassifier):
                param_grid = {
                "preprocessor__vect_resp_text__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "preprocessor__vect_pos__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "estimator__n_estimators": np.arange(20,150), 
                "estimator__max_features": ['log2', 'sqrt'],
                "estimator__max_depth": np.arange(10,110),
                "estimator__min_samples_split": np.arange(2,11),
                "estimator__min_samples_leaf": np.arange(1,5),
                "estimator__bootstrap": [True, False]
                }
                
            if isinstance(model, XGBClassifier):
                param_grid = {
                "preprocessor__vect_resp_text__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "preprocessor__vect_pos__ngram_range": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],
                "estimator__gamma": np.linspace(0,9,100),
                "estimator__alpha": np.linspace(0,40,100),
                "estimator__lambda": np.linspace(0,3,10),
                "estimator__colsample_bytree": np.linspace(0.2,1,10),
                "estimator__min_child_weight": np.linspace(0,10,100),
                }

                
            # Prints do modelo e da vetorização
            if isinstance(model, LogisticRegression):
                if 'l1' in model.get_params()['penalty']:
                    print(f'Modelo: {model} l1')
                else:
                    print(f'Modelo: {model} l2')
            else:
                print(f'Modelo: {model}')
                
            print(f'Vetorizador utilizado: {vectorizer}')
            
            # Random Search
            comeco_random_search = datetime.datetime.now()
            print(f'Começo da Random Search: {comeco_random_search}')
                
            random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid,cv=StratifiedKFold(n_splits=5),
                                                n_iter=iteracoes, n_jobs=2, random_state=24)
            
            model_trained = random_search.fit(X_train, y_train)

            final_random_search = datetime.datetime.now()
            print(f'Final da Random Search: {final_random_search}')

            print(f'get_params: {model_trained.get_params}')
            
            score_random_search = model_trained.best_score_
            score_random_search *= 100
            score_random_search = round(score_random_search,2)
            print(f'Melhor resultado na Random Search: {score_random_search}%')
            
            print('Melhores parâmetros encontrados:')
            print(model_trained.best_params_)
            
            # Predição
            y_pred = model_trained.predict(X_test)
            acc_pred = accuracy_score(y_test, y_pred)
            acc_pred *= 100
            acc_pred = round(acc_pred,2)
            print(f'Acurácia predita = {acc_pred}%')
                    
            # Avaliação Ivandre

            pipeline = Pipeline([
                    ('preprocessor', preprocessor), 
                    ('scaling', scaler), 
                    ('estimator', model)
                    ])

            pipeline = pipeline.set_params(**model_trained.best_params_)

            print(f'get_params: {pipeline.get_params}')
            
            comeco_av_iv = datetime.datetime.now()
            print(f'Começo da Avaliação Ivandre: {comeco_av_iv}')
            acc_iv = cross_val_score(pipeline, X, y, scoring='accuracy', cv=10, n_jobs=2).mean()
            acc_iv *= 100
            acc_iv = round(acc_iv,2)
            final_av_iv = datetime.datetime.now()
            print(f'Final da Avaliação Ivandre: {final_av_iv}')
        
            print(f'Acurácia Ivandre = {acc_iv}%')
            
            print('----------------------------------------------')
            
            # Escrita no arquivo
            with open('compara-nb-svc.txt', "a") as arquivo:
                
                if isinstance(model, LogisticRegression):
                    if 'l1' in model.get_params()['penalty']:
                        arquivo.write(f'Modelo: {model} l1\n')
                    else:
                        arquivo.write(f'Modelo: {model} l2\n')
                else:
                    arquivo.write(f'Modelo: {model}\n')

                arquivo.write(f'Vetorizador utilizado: {vectorizer}\n')
                
                arquivo.write(f'Começo da Random Search: {comeco_random_search}\n')
                arquivo.write(f'Final da Random Search: {final_random_search}\n')
                arquivo.write(f'Melhor resultado na Random Search: {score_random_search}\n')
                arquivo.write('Melhores parâmetros encontrados:\n')
                arquivo.write(str(model_trained.best_params_))
                arquivo.write('\n')
                arquivo.write(f'Acurácia predita = {acc_pred}%\n')
                arquivo.write(f'Começo da Avaliação Ivandre: {comeco_av_iv}\n')
                arquivo.write(f'Final da Avaliação Ivandre: {final_av_iv}\n')
                arquivo.write(f'Acurácia Ivandre = {acc_iv}%\n')
                arquivo.write('----------------------------------------------\n')


compara(1)


model = MultinomialNB(fit_prior= False)

scaler = MaxAbsScaler()

preprocessor = ColumnTransformer(transformers=[
                                            ('vect_resp_text', CountVectorizer(analyzer='word'),'resp_text'), 
                                            ('vect_pos', CountVectorizer(analyzer='word'),'pos')])

pipeline = Pipeline([
                    ('preprocessor', preprocessor), 
                    ('scaling', scaler), 
                    ('estimator', model)
                    ])

pipeline = pipeline.set_params(**model_trained.best_params_)

print(f'get_params: {pipeline.get_params}')



