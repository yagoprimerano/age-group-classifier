{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce62debc-bf37-4d18-85a9-dbd1069ebb20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importação das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22df3ca1-3dbe-4119-a257-9e8412799283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import  classification_report\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40afdead-65da-429b-a50f-0502db5333fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c89ff5c-9986-4808-bcf4-8ef62fbaf133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import RFECV, RFE, SelectKBest\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "from sklearn.svm import SVC\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726dd049-2914-4e15-aa97-41f875bc5e6d",
   "metadata": {},
   "source": [
    "# Importação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f094c27-9803-45b7-927f-418b6a45133e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('../data/(CORRIGIDO)ep2_pln_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b506f40-5b6f-47c4-9ed9-5def04c2904b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_text</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- Quantos empregados em cada um dos atuais nív...</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- Solicito cópia das Atas do Conselho de Admin...</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Solicito informar a norma (lei, decreto, por...</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Solicito por gentileza, a informação sobre a...</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Solicito por gentileza, a informação sobre a...</td>\n",
       "      <td>a2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            req_text age\n",
       "0  - Quantos empregados em cada um dos atuais nív...  a2\n",
       "1  - Solicito cópia das Atas do Conselho de Admin...  a2\n",
       "2  - Solicito informar a norma (lei, decreto, por...  a2\n",
       "3  - Solicito por gentileza, a informação sobre a...  a2\n",
       "4  - Solicito por gentileza, a informação sobre a...  a2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() # visualização das primeiras 5 linhas do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f7a6c0-5b3a-4e1d-88aa-085d5b93d9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8200, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # visualização do formato do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c6d53f-17cb-4a87-8377-0fc88daf9fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age\n",
       "a3    3000\n",
       "a2    2000\n",
       "a4    2000\n",
       "a1    1200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ef30cb-c985-4221-b0fb-48e617b6732c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['age'] = data['age'].map({\n",
    "                                'a4':3,\n",
    "                                'a3': 2,\n",
    "                                'a2': 1,\n",
    "                                'a1': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4910bc1c-f52a-4bae-985f-a5282c1729a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>req_text</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- Quantos empregados em cada um dos atuais nív...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- Solicito cópia das Atas do Conselho de Admin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Solicito informar a norma (lei, decreto, por...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Solicito por gentileza, a informação sobre a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Solicito por gentileza, a informação sobre a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            req_text  age\n",
       "0  - Quantos empregados em cada um dos atuais nív...    1\n",
       "1  - Solicito cópia das Atas do Conselho de Admin...    1\n",
       "2  - Solicito informar a norma (lei, decreto, por...    1\n",
       "3  - Solicito por gentileza, a informação sobre a...    1\n",
       "4  - Solicito por gentileza, a informação sobre a...    1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c659419-4e04-45cc-88c3-dbbeec61d993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a seguir os dados serão divididos entre features (X) e label (y)\n",
    "X = data['req_text'] # features\n",
    "y = data['age'] # label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb5652-63fb-4fb7-b59f-16c9fbe8699a",
   "metadata": {},
   "source": [
    "# Seleção de hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383f61a-c05a-4c82-bdda-95a7856198ab",
   "metadata": {},
   "source": [
    "Nesta seção, será feita a comparação entre classificadores com os melhores hiperparâmetros encontrados através da utilização do otimizador Optuna, que, resumidamente, funciona de X forma. Os classificadores comparados são:\n",
    "- Regressão Logística\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "114a5435-f249-4ab4-aa5d-eb8adb732470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2843a037-0eae-458f-bca9-296fb683a646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d4728-aaa3-4fed-8495-6a2feacba516",
   "metadata": {},
   "source": [
    "# Seleção do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a72d42fd-ae47-4426-873d-24812b262f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleciona_grid(model):\n",
    "\n",
    "    param_grid = None\n",
    "\n",
    "    if isinstance(model, MultinomialNB):\n",
    "            param_grid = {\n",
    "            \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "            \"vect__analyzer\": ['word','char'],\n",
    "            \"selection__percentile\": [33, 66, 100],\n",
    "            \"estimator__alpha\": [50, 15, 10, 5, 1, 0.5, 0.3, 0.1, 0.05, 0.03, 0.02, 0.01,  0.001],\n",
    "            \"estimator__fit_prior\": [True, False],\n",
    "            }\n",
    "\n",
    "    if isinstance(model, SVC):\n",
    "            param_grid = {\n",
    "            \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "            \"vect__analyzer\": ['word','char'],\n",
    "            \"selection__percentile\": [33, 66, 100],\n",
    "            \"estimator__gamma\": [1, 0.1, 0.01, 0.001],\n",
    "            \"estimator__kernel\": ['linear', 'sigmoid'],\n",
    "            \"estimator__C\": [0.1, 1, 10, 100]\n",
    "            }\n",
    "            \n",
    "    if isinstance(model, LinearSVC):\n",
    "            param_grid = {\n",
    "            \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "            \"vect__analyzer\": ['word','char'],\n",
    "            \"selection__percentile\": [33, 66, 100],\n",
    "            \"estimator__dual\": [True, False],\n",
    "            \"estimator__penalty\": ['l1', 'l2'],\n",
    "            \"estimator__fit_intercept\": [True, False],\n",
    "            \"estimator__C\": uniform(loc=0, scale=4)\n",
    "            }\n",
    "\n",
    "\n",
    "    if isinstance(model, RandomForestClassifier):\n",
    "        param_grid = {\n",
    "            \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "            \"vect__analyzer\": ['word','char'], \n",
    "            \"selection__percentile\": [33, 66, 100],\n",
    "            \"estimator__n_estimators\": np.arange(20,150), \n",
    "            \"estimator__max_features\": ['log2', 'sqrt'],\n",
    "            \"estimator__max_depth\": np.arange(10,110),\n",
    "            \"estimator__min_samples_split\": np.arange(2,11),\n",
    "            \"estimator__min_samples_leaf\": np.arange(1,5),\n",
    "            \"estimator__bootstrap\": [True, False]\n",
    "            }\n",
    "        \n",
    "    if isinstance(model, XGBClassifier):\n",
    "        param_grid = {\n",
    "            \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "            \"vect__analyzer\": ['word','char'],\n",
    "            \"selection__percentile\": [33, 66, 100],\n",
    "            \"estimator__gamma\": np.linspace(0,9,100, dtype=np.int64),\n",
    "            \"estimator__alpha\": np.linspace(0,40,100, dtype=np.int64),\n",
    "            \"estimator__lambda\": np.linspace(0,3,10, dtype=np.int64),\n",
    "            \"estimator__colsample_bytree\": np.linspace(0.2,1,10, dtype=np.int64)\n",
    "            }\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04101a57-b165-4caa-a663-3f3395026b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_e_avalia(random_search):\n",
    "    \n",
    "    inicio_random_search = datetime.datetime.now()\n",
    "\n",
    "    model_trained = random_search.fit(X_train, y_train) # fit\n",
    "\n",
    "    fim_random_search = datetime.datetime.now()\n",
    "    tempo_total = fim_random_search - inicio_random_search\n",
    "    print(f'Duração da Random Search: {tempo_total}')\n",
    "\n",
    "    y_pred = model_trained.predict(X_test) # predicao\n",
    "\n",
    "    # Predição F1 e Class Report\n",
    "    f1 = f1_score(y_test, y_pred, average= 'macro') # f1\n",
    "    f1 *= 100\n",
    "    f1 = round(f1,2)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True) # class report\n",
    "    \n",
    "    return model_trained, tempo_total, f1, report\n",
    "\n",
    "    #######\n",
    "    \n",
    "    #if split == 'strat_vies': # estratificacao pela label\n",
    "    #    model_trained = random_search.fit(X_train_strat_vies, y_train_strat_vies) # fit\n",
    "\n",
    "    #    fim_random_search = datetime.datetime.now()\n",
    "    #    tempo_total = fim_random_search - inicio_random_search\n",
    "    #    print(f'Duração da Random Search: {tempo_total}')\n",
    "\n",
    "    #    y_pred = model_trained.predict(X_test_strat_vies) # predicao\n",
    "    #    f1 = f1_score(y_test_strat_vies, y_pred, average= 'macro') # f1\n",
    "    #    report = classification_report(y_test_strat_vies, y_pred, output_dict=True) # class report\n",
    "\n",
    "    #elif split == 'strat_partido': # estratificacao pelos partidos \n",
    "    #    model_trained = random_search.fit(X_train_strat_part, y_train_strat_part) # fit\n",
    "\n",
    "    #   fim_random_search = datetime.datetime.now()\n",
    "    #    tempo_total = fim_random_search - inicio_random_search\n",
    "    #    print(f'Duração da Random Search: {tempo_total}')\n",
    "\n",
    "    #    y_pred = model_trained.predict(X_test_strat_part) # predicao\n",
    "    #    f1 = f1_score(y_test_strat_part, y_pred, average= 'macro') # f1\n",
    "    #    report = classification_report(y_test_strat_part, y_pred, output_dict=True) # class report\n",
    "\n",
    "    #elif split == 'pred_partido_novo': # predicao de partidos nao vistos no teste\n",
    "     #   model_trained = random_search.fit(X_train_part_novos, y_train_part_novos) # fit\n",
    "\n",
    "     #   fim_random_search = datetime.datetime.now()\n",
    "     #   tempo_total = fim_random_search - inicio_random_search\n",
    "     #   print(f'Duração da Random Search: {tempo_total}')\n",
    "\n",
    "     #   y_pred = model_trained.predict(X_test_part_novos) # predicao\n",
    "     #   f1 = f1_score(y_test_part_novos, y_pred, average= 'macro') # f1\n",
    "     #   report = classification_report(y_test_part_novos, y_pred, output_dict=True) # class report\n",
    "    \n",
    "    #return model_trained, tempo_total, f1, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32055d7b-f151-4e90-bf8a-3c8da39efe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compara(iteracoes, modelos, nome_arquivo):\n",
    "\n",
    "    # seletor de features\n",
    "    selection = SelectPercentile()\n",
    "\n",
    "    # possibilidades de oversampling ou nao\n",
    "    samplers = [RandomOverSampler(random_state=42), None]\n",
    "\n",
    "    # diferentes splits que serao avaliador\n",
    "    #splits = ['strat_vies', 'strat_partido', 'pred_partido_novo'] \n",
    "\n",
    "    # dataframe em que sera inserido os dados do modelo testado \n",
    "    df_resultados = pd.DataFrame(columns=['modelo', 'vect','sampler', 'scaling',\n",
    "                                          'duracao_random_search','qnt_iteracoes',\n",
    "                                          'f1_randsearch', \n",
    "                                           'f1_pred',\n",
    "                                          'class_report',\n",
    "                                         'duracao_aval_iv',\n",
    "                                          'acc_aval_iv',\n",
    "                                         'melhores_parametros'])\n",
    "    \n",
    "    for model in modelos:\n",
    "\n",
    "        for sampler in samplers:\n",
    "\n",
    "            #for split in splits:\n",
    "                \n",
    "            # seleciona grid de parametros\n",
    "            param_grid = seleciona_grid(model)\n",
    "            \n",
    "            \n",
    "            scaler = MaxAbsScaler()\n",
    "    \n",
    "            # define o pipeline\n",
    "            pipeline = Pipeline([\n",
    "                    ('vect', TfidfVectorizer()),\n",
    "                    ('scaling', scaler), \n",
    "                    ('selection', selection),\n",
    "                    ('ros', sampler),\n",
    "                    ('estimator', model)\n",
    "                    ])\n",
    "    \n",
    "            \n",
    "            #  --- Prints das configurações dessa iteracao ---\n",
    "            print(f'Modelo: {model}')\n",
    "            #print(f'Split: {split}')\n",
    "            print(f'Scaler: {scaler}')\n",
    "            print(f'Sampler: {sampler}')\n",
    "                \n",
    "    \n",
    "            # definicao da randomized search\n",
    "            random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid,cv=StratifiedKFold(n_splits=5),\n",
    "                                                n_iter=iteracoes, n_jobs=1, random_state=42, scoring='f1_macro')\n",
    "\n",
    "\n",
    "            # fit e avaliacao pela randomized search\n",
    "            model_trained, tempo_total, f1, report = fit_e_avalia(random_search)\n",
    "                \n",
    "            print('---')\n",
    "            resultados = model_trained.cv_results_\n",
    "\n",
    "            for params, score in zip(resultados['params'], resultados['mean_test_score']):\n",
    "                print(f\"Parâmetros: {params}, Score: {score}\")\n",
    "            print('---')    \n",
    "                \n",
    "            # melhor metrica na random search\n",
    "            score_random_search = model_trained.best_score_\n",
    "            score_random_search *= 100\n",
    "            score_random_search = round(score_random_search,2)\n",
    "            print(f'Melhor F1 na Random Search: {score_random_search}%')\n",
    "            \n",
    "            # melhores parametros encontrados\n",
    "            print('Melhores parâmetros encontrados:')\n",
    "            print(model_trained.best_params_)\n",
    "\n",
    "            \n",
    "            # acuracia da predicao\n",
    "            print(f'F1 macro = {f1}%')\n",
    "    \n",
    "            # classification report\n",
    "            print(report)\n",
    "\n",
    "            # Avaliação Ivandre\n",
    "            pipeline = Pipeline([\n",
    "                            ('vect', TfidfVectorizer()),\n",
    "                            ('scaling', scaler), \n",
    "                            ('selection', selection),\n",
    "                            ('ros', sampler),\n",
    "                            ('estimator', model)\n",
    "                            ])\n",
    "        \n",
    "            pipeline = pipeline.set_params(**model_trained.best_params_)\n",
    "        \n",
    "            print(f'get_params: {pipeline.get_params}')\n",
    "            \n",
    "            inicio_aval_iv = datetime.datetime.now()\n",
    "            acc_iv = cross_val_score(pipeline, X, y, scoring='accuracy', cv=10, n_jobs=2).mean()\n",
    "            acc_iv *= 100\n",
    "            acc_iv = round(acc_iv,2)\n",
    "            fim_aval_iv = datetime.datetime.now()\n",
    "            tempo_aval_iv = fim_aval_iv - inicio_aval_iv\n",
    "            print(f'Duração da Avaliação Ivandre: {tempo_aval_iv}')\n",
    "        \n",
    "            print(f'Acurácia Ivandre = {acc_iv}%')\n",
    "                    \n",
    "            \n",
    "            print('----------------------------------------------')\n",
    "            \n",
    "            # --- Escrita em memória secundária ---\n",
    "\n",
    "            # Nova linha que sera adicionada\n",
    "            nova_linha = {'modelo': model, 'vect': TfidfVectorizer(),\n",
    "                          'sampler': str(sampler), 'scaling': scaler,\n",
    "                          'duracao_random_search': tempo_total,\n",
    "                          'qnt_iteracoes': iteracoes,\n",
    "                          'f1_randsearch': f'{score_random_search}%',\n",
    "                          'f1_pred': f'{f1}%', 'class_report': report,\n",
    "                           'duracao_aval_iv': tempo_aval_iv,\n",
    "                           'acc_aval_iv': f'{acc_iv}%',\n",
    "                           'melhores_parametros': str(model_trained.best_params_)}\n",
    "        \n",
    "            # Cria um novo DataFrame com a nova linha\n",
    "            nova_linha_resultados = pd.DataFrame([nova_linha])\n",
    "        \n",
    "            # Concatena o novo DataFrame com o DataFrame existente\n",
    "            df_resultados = pd.concat([df_resultados, nova_linha_resultados], ignore_index=True)\n",
    "\n",
    "            # salvamento do dataframe de resultados apos os testes terem terminado\n",
    "            df_resultados.to_csv(nome_arquivo, index=False)\n",
    "    \n",
    "    \n",
    "    print('Fim dos testes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0bde26-b65b-41f7-af08-268f7875e815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: MultinomialNB()\n",
      "Scaler: MaxAbsScaler()\n",
      "Sampler: RandomOverSampler(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "modelos = [MultinomialNB()]\n",
    "\n",
    "compara(240, modelos, 'compara-nb-240-.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f801d6-faa9-4cf9-ad72-5696a849a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcb47a-8444-4f1f-828b-9a68914b6f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d7986-e657-4bd4-9906-6af0e66c0206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa7f3f-4a9a-47cb-a926-91069188e7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319ae76-aad6-44dd-97fe-dc2a07159af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31550b8e-4684-401b-b4ac-e1cb5f04ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compara(iteracoes):\n",
    "    \n",
    "    # Escrita no arquivo\n",
    "    with open('compara-xg-rf.txt', \"w\") as arquivo:\n",
    "        pass\n",
    "    \n",
    "    modelos = [XGBClassifier(seed=42),RandomForestClassifier(random_state=42)]\n",
    "    \n",
    "    vect = [CountVectorizer(analyzer='word'), TfidfVectorizer(analyzer= 'word')]\n",
    "    \n",
    "    for model in modelos:\n",
    "        \n",
    "        for vectorizer in vect:\n",
    "    \n",
    "            param_grid = None\n",
    "            \n",
    "            scaler = MaxAbsScaler()\n",
    "\n",
    "            preprocessor = ColumnTransformer(transformers=[\n",
    "                                            ('vect_resp_text', vectorizer,'resp_text'), \n",
    "                                            ('vect_pos', vectorizer,'pos')])\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                    ('preprocessor', preprocessor), \n",
    "                    ('scaling', scaler), \n",
    "                    ('estimator', model)\n",
    "                    ])\n",
    "\n",
    "            if isinstance(model, MultinomialNB):\n",
    "                param_grid = {\n",
    "                \"preprocessor__vect_resp_text__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"preprocessor__vect_pos__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"estimator__alpha\": [50, 15, 10, 5, 1, 0.5, 0.3, 0.1, 0.05, 0.03, 0.02, 0.01,  0.001],\n",
    "                \"estimator__fit_prior\": [True, False],\n",
    "                }\n",
    "\n",
    "            if isinstance(model, SVC):\n",
    "                param_grid = {\n",
    "                \"preprocessor__vect_resp_text__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"preprocessor__vect_pos__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"estimator__gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                \"estimator__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                \"estimator__C\": [0.1, 1, 10, 100, 1000]\n",
    "                }\n",
    "\n",
    "            if isinstance(model, LogisticRegression):\n",
    "\n",
    "                if 'l1' in model.get_params()['penalty']:\n",
    "                    param_grid = {\n",
    "                    \"preprocessor__vect_resp_text__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                    \"preprocessor__vect_pos__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                    \"estimator__C\": np.linspace(0,10,100),\n",
    "                    \"estimator__solver\": ['liblinear','saga']\n",
    "                    }\n",
    "\n",
    "                elif 'l2' in model.get_params()['penalty']:\n",
    "                    param_grid = {\n",
    "                    \"preprocessor__vect_resp_text__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                    \"preprocessor__vect_pos__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                    \"estimator__C\": np.linspace(0,10,100),\n",
    "                    \"estimator__solver\": ['lbfgs', 'newton-cg', 'newton-cholesky',\n",
    "                                          'liblinear','saga', 'sag']\n",
    "                    }\n",
    "\n",
    "            if isinstance(model, RandomForestClassifier):\n",
    "                param_grid = {\n",
    "                \"preprocessor__vect_resp_text__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"preprocessor__vect_pos__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"estimator__n_estimators\": np.arange(20,150), \n",
    "                \"estimator__max_features\": ['log2', 'sqrt'],\n",
    "                \"estimator__max_depth\": np.arange(10,110),\n",
    "                \"estimator__min_samples_split\": np.arange(2,11),\n",
    "                \"estimator__min_samples_leaf\": np.arange(1,5),\n",
    "                \"estimator__bootstrap\": [True, False]\n",
    "                }\n",
    "                \n",
    "            if isinstance(model, XGBClassifier):\n",
    "                param_grid = {\n",
    "                \"preprocessor__vect_resp_text__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"preprocessor__vect_pos__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"estimator__gamma\": np.linspace(0,9,100, dtype=np.int64),\n",
    "                \"estimator__alpha\": np.linspace(0,40,100, dtype=np.int64),\n",
    "                \"estimator__lambda\": np.linspace(0,3,10, dtype=np.int64),\n",
    "                \"estimator__colsample_bytree\": np.linspace(0.2,1,10, dtype=np.int64)\n",
    "                }\n",
    "\n",
    "                \n",
    "                \n",
    "            # Prints do modelo e da vetorização\n",
    "            if isinstance(model, LogisticRegression):\n",
    "                if 'l1' in model.get_params()['penalty']:\n",
    "                    print(f'Modelo: {model} l1')\n",
    "                else:\n",
    "                    print(f'Modelo: {model} l2')\n",
    "            else:\n",
    "                print(f'Modelo: {model}')\n",
    "                \n",
    "            print(f'Vetorizador utilizado: {vectorizer}')\n",
    "            \n",
    "            # Random Search\n",
    "            comeco_random_search = datetime.datetime.now()\n",
    "            print(f'Começo da Random Search: {comeco_random_search}')\n",
    "                \n",
    "            random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid,cv=StratifiedKFold(n_splits=5),\n",
    "                                                n_iter=iteracoes, n_jobs=2, random_state=24)\n",
    "            \n",
    "            model_trained = random_search.fit(X_train, y_train)\n",
    "\n",
    "            final_random_search = datetime.datetime.now()\n",
    "            print(f'Final da Random Search: {final_random_search}')\n",
    "            \n",
    "            score_random_search = model_trained.best_score_\n",
    "            score_random_search *= 100\n",
    "            score_random_search = round(score_random_search,2)\n",
    "            print(f'Melhor resultado na Random Search: {score_random_search}%')\n",
    "            \n",
    "            print('Melhores parâmetros encontrados:')\n",
    "            print(model_trained.best_params_)\n",
    "            \n",
    "            # Predição\n",
    "            y_pred = model_trained.predict(X_test)\n",
    "            acc_pred = accuracy_score(y_test, y_pred)\n",
    "            acc_pred *= 100\n",
    "            acc_pred = round(acc_pred,2)\n",
    "            print(f'Acurácia predita = {acc_pred}%')\n",
    "                    \n",
    "            # Avaliação Ivandre\n",
    "            comeco_av_iv = datetime.datetime.now()\n",
    "            print(f'Começo da Avaliação Ivandre: {comeco_av_iv}')\n",
    "            acc_iv = cross_val_score(model_trained, X, y, scoring='accuracy', cv=10, n_jobs=2).mean()\n",
    "            acc_iv *= 100\n",
    "            acc_iv = round(acc_iv,2)\n",
    "            final_av_iv = datetime.datetime.now()\n",
    "            print(f'Final da Avaliação Ivandre: {final_av_iv}')\n",
    "        \n",
    "            print(f'Acurácia Ivandre = {acc_iv}%')\n",
    "            \n",
    "            print('----------------------------------------------')\n",
    "            \n",
    "            # Escrita no arquivo\n",
    "            with open('compara-xg-rf.txt', \"a\") as arquivo:\n",
    "                \n",
    "                if isinstance(model, LogisticRegression):\n",
    "                    if 'l1' in model.get_params()['penalty']:\n",
    "                        arquivo.write(f'Modelo: {model} l1\\n')\n",
    "                    else:\n",
    "                        arquivo.write(f'Modelo: {model} l2\\n')\n",
    "                else:\n",
    "                    arquivo.write(f'Modelo: {model}\\n')\n",
    "\n",
    "                arquivo.write(f'Vetorizador utilizado: {vectorizer}\\n')\n",
    "                \n",
    "                arquivo.write(f'Começo da Random Search: {comeco_random_search}\\n')\n",
    "                arquivo.write(f'Final da Random Search: {final_random_search}\\n')\n",
    "                arquivo.write(f'Melhor resultado na Random Search: {score_random_search}\\n')\n",
    "                arquivo.write('Melhores parâmetros encontrados:\\n')\n",
    "                arquivo.write(str(model_trained.best_params_))\n",
    "                arquivo.write('\\n')\n",
    "                arquivo.write(f'Acurácia predita = {acc_pred}%\\n')\n",
    "                arquivo.write(f'Começo da Avaliação Ivandre: {comeco_av_iv}\\n')\n",
    "                arquivo.write(f'Final da Avaliação Ivandre: {final_av_iv}\\n')\n",
    "                arquivo.write(f'Acurácia Ivandre = {acc_iv}%\\n')\n",
    "                arquivo.write('----------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c646da-ded2-42f1-91b6-6ecf392bddbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compara(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
